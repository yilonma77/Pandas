{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0d0c873",
   "metadata": {},
   "source": [
    "# üåè Scraper de Changements R√©glementaires - March√©s Asiatiques\n",
    "\n",
    "Ce notebook scrape les changements r√©glementaires des march√©s boursiers de:\n",
    "- üáπüá≠ **Tha√Ølande** (SEC Thailand)\n",
    "- üá∏üá¨ **Singapour** (MAS - Monetary Authority of Singapore)\n",
    "- üá≤üáæ **Malaisie** (SC Malaysia)\n",
    "\n",
    "## üìã M√©thodologie\n",
    "- Utilisation de **Selenium** pour les sites dynamiques\n",
    "- **BeautifulSoup** pour le parsing HTML\n",
    "- **Pandas** pour l'analyse des donn√©es\n",
    "- Export vers CSV et Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162d0fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des biblioth√®ques essentielles\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Selenium pour le scraping dynamique\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "# BeautifulSoup pour le parsing\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Requests pour les requ√™tes HTTP simples\n",
    "import requests\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques import√©es avec succ√®s!\")\n",
    "print(f\"üìÖ Date d'ex√©cution: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f2fc0",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration du Driver Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9fd081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_driver(headless=True):\n",
    "    \"\"\"\n",
    "    Configure le driver Selenium Chrome\n",
    "    \n",
    "    Args:\n",
    "        headless: Si True, ex√©cute le navigateur en mode headless (sans interface)\n",
    "    \n",
    "    Returns:\n",
    "        driver: Instance du WebDriver Chrome\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    \n",
    "    if headless:\n",
    "        chrome_options.add_argument('--headless')\n",
    "    \n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--window-size=1920,1080')\n",
    "    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36')\n",
    "    \n",
    "    # D√©sactiver les notifications\n",
    "    chrome_options.add_experimental_option('prefs', {\n",
    "        'profile.default_content_setting_values.notifications': 2\n",
    "    })\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.implicitly_wait(10)\n",
    "    \n",
    "    return driver\n",
    "\n",
    "print(\"‚úÖ Fonction de configuration du driver cr√©√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbc7c32",
   "metadata": {},
   "source": [
    "## üáπüá≠ 1. Scraping - SEC Thailand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d9a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_sec_thailand():\n",
    "    \"\"\"\n",
    "    Scrape les changements r√©glementaires de la SEC Thailand\n",
    "    Site: https://www.sec.or.th/EN/Pages/News_Detail.aspx\n",
    "    \"\"\"\n",
    "    print(\"üáπüá≠ Scraping SEC Thailand...\")\n",
    "    \n",
    "    url = \"https://www.sec.or.th/EN/Pages/News_All.aspx\"\n",
    "    \n",
    "    try:\n",
    "        # Utiliser requests pour cette page (plus rapide)\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        regulations = []\n",
    "        \n",
    "        # Recherche des articles de news/r√©gulations\n",
    "        news_items = soup.find_all(['div', 'article', 'li'], class_=re.compile('news|item|article', re.I))\n",
    "        \n",
    "        if not news_items:\n",
    "            # Alternative: chercher tous les liens avec dates\n",
    "            news_items = soup.find_all('a', href=True)\n",
    "        \n",
    "        for item in news_items[:30]:  # Limiter aux 30 premiers\n",
    "            try:\n",
    "                # Extraction du titre\n",
    "                title_elem = item.find(['h2', 'h3', 'h4', 'a']) or item\n",
    "                title = title_elem.get_text(strip=True)\n",
    "                \n",
    "                # Filter pour les r√©gulations\n",
    "                if len(title) < 10 or not any(word in title.lower() for word in \n",
    "                    ['regulation', 'rule', 'law', 'act', 'requirement', 'guideline', 'circular']):\n",
    "                    continue\n",
    "                \n",
    "                # Extraction de la date\n",
    "                date_text = item.get_text()\n",
    "                date_match = re.search(r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\d{4}[/-]\\d{1,2}[/-]\\d{1,2}', date_text)\n",
    "                date = date_match.group(0) if date_match else 'N/A'\n",
    "                \n",
    "                # Extraction du lien\n",
    "                link_elem = item.find('a', href=True) or (item if item.name == 'a' else None)\n",
    "                link = link_elem['href'] if link_elem else ''\n",
    "                if link and not link.startswith('http'):\n",
    "                    link = 'https://www.sec.or.th' + link\n",
    "                \n",
    "                regulations.append({\n",
    "                    'Country': 'Thailand üáπüá≠',\n",
    "                    'Authority': 'SEC Thailand',\n",
    "                    'Title': title,\n",
    "                    'Date': date,\n",
    "                    'Link': link,\n",
    "                    'Scraped_At': datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "                })\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        print(f\"‚úÖ SEC Thailand: {len(regulations)} r√©gulations trouv√©es\")\n",
    "        return regulations\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur SEC Thailand: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test de la fonction\n",
    "thailand_data = scrape_sec_thailand()\n",
    "if thailand_data:\n",
    "    df_thailand = pd.DataFrame(thailand_data)\n",
    "    print(\"\\nüìä Aper√ßu des donn√©es Tha√Ølande:\")\n",
    "    display(df_thailand.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedd0993",
   "metadata": {},
   "source": [
    "## üá∏üá¨ 2. Scraping - MAS Singapore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a06b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_mas_singapore():\n",
    "    \"\"\"\n",
    "    Scrape les changements r√©glementaires de MAS Singapore\n",
    "    Site: https://www.mas.gov.sg/news\n",
    "    \"\"\"\n",
    "    print(\"üá∏üá¨ Scraping MAS Singapore...\")\n",
    "    \n",
    "    url = \"https://www.mas.gov.sg/news/regulations-and-circulars\"\n",
    "    \n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        regulations = []\n",
    "        \n",
    "        # Recherche des √©l√©ments de r√©gulation\n",
    "        items = soup.find_all(['div', 'article', 'li'], class_=re.compile('card|item|news|article', re.I))\n",
    "        \n",
    "        if not items:\n",
    "            items = soup.find_all('a', href=re.compile('regulation|circular|notice', re.I))\n",
    "        \n",
    "        for item in items[:30]:\n",
    "            try:\n",
    "                # Titre\n",
    "                title_elem = item.find(['h2', 'h3', 'h4', 'span', 'a'])\n",
    "                if not title_elem:\n",
    "                    continue\n",
    "                title = title_elem.get_text(strip=True)\n",
    "                \n",
    "                if len(title) < 10:\n",
    "                    continue\n",
    "                \n",
    "                # Date\n",
    "                date_elem = item.find(['time', 'span'], class_=re.compile('date', re.I))\n",
    "                if date_elem:\n",
    "                    date = date_elem.get_text(strip=True)\n",
    "                else:\n",
    "                    date_match = re.search(r'\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{4}', item.get_text())\n",
    "                    date = date_match.group(0) if date_match else 'N/A'\n",
    "                \n",
    "                # Lien\n",
    "                link_elem = item.find('a', href=True)\n",
    "                link = link_elem['href'] if link_elem else ''\n",
    "                if link and not link.startswith('http'):\n",
    "                    link = 'https://www.mas.gov.sg' + link\n",
    "                \n",
    "                regulations.append({\n",
    "                    'Country': 'Singapore üá∏üá¨',\n",
    "                    'Authority': 'MAS',\n",
    "                    'Title': title,\n",
    "                    'Date': date,\n",
    "                    'Link': link,\n",
    "                    'Scraped_At': datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        print(f\"‚úÖ MAS Singapore: {len(regulations)} r√©gulations trouv√©es\")\n",
    "        return regulations\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur MAS Singapore: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test de la fonction\n",
    "singapore_data = scrape_mas_singapore()\n",
    "if singapore_data:\n",
    "    df_singapore = pd.DataFrame(singapore_data)\n",
    "    print(\"\\nüìä Aper√ßu des donn√©es Singapour:\")\n",
    "    display(df_singapore.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24ce252",
   "metadata": {},
   "source": [
    "## üá≤üáæ 3. Scraping - SC Malaysia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a8992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_sc_malaysia():\n",
    "    \"\"\"\n",
    "    Scrape les changements r√©glementaires de SC Malaysia\n",
    "    Site: https://www.sc.com.my\n",
    "    \"\"\"\n",
    "    print(\"üá≤üáæ Scraping SC Malaysia...\")\n",
    "    \n",
    "    url = \"https://www.sc.com.my/regulation/guidelines\"\n",
    "    \n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        regulations = []\n",
    "        \n",
    "        # Recherche des items\n",
    "        items = soup.find_all(['div', 'article', 'li', 'tr'], class_=re.compile('item|card|row|news', re.I))\n",
    "        \n",
    "        if not items:\n",
    "            items = soup.find_all('a', href=True)\n",
    "        \n",
    "        for item in items[:30]:\n",
    "            try:\n",
    "                # Titre\n",
    "                title_elem = item.find(['h2', 'h3', 'h4', 'td', 'span', 'a'])\n",
    "                if not title_elem:\n",
    "                    continue\n",
    "                title = title_elem.get_text(strip=True)\n",
    "                \n",
    "                if len(title) < 10:\n",
    "                    continue\n",
    "                \n",
    "                # Date\n",
    "                date_elem = item.find(['time', 'span', 'td'], class_=re.compile('date', re.I))\n",
    "                if date_elem:\n",
    "                    date = date_elem.get_text(strip=True)\n",
    "                else:\n",
    "                    date_match = re.search(r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\d{4}[/-]\\d{1,2}[/-]\\d{1,2}', item.get_text())\n",
    "                    date = date_match.group(0) if date_match else 'N/A'\n",
    "                \n",
    "                # Lien\n",
    "                link_elem = item.find('a', href=True)\n",
    "                link = link_elem['href'] if link_elem else ''\n",
    "                if link and not link.startswith('http'):\n",
    "                    link = 'https://www.sc.com.my' + link\n",
    "                \n",
    "                regulations.append({\n",
    "                    'Country': 'Malaysia üá≤üáæ',\n",
    "                    'Authority': 'SC Malaysia',\n",
    "                    'Title': title,\n",
    "                    'Date': date,\n",
    "                    'Link': link,\n",
    "                    'Scraped_At': datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        print(f\"‚úÖ SC Malaysia: {len(regulations)} r√©gulations trouv√©es\")\n",
    "        return regulations\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur SC Malaysia: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test de la fonction\n",
    "malaysia_data = scrape_sc_malaysia()\n",
    "if malaysia_data:\n",
    "    df_malaysia = pd.DataFrame(malaysia_data)\n",
    "    print(\"\\nüìä Aper√ßu des donn√©es Malaisie:\")\n",
    "    display(df_malaysia.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef9a68d",
   "metadata": {},
   "source": [
    "## üîÑ 4. Scraping avec Selenium (pour sites dynamiques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d73016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_with_selenium(url, country, authority):\n",
    "    \"\"\"\n",
    "    Scraping g√©n√©rique avec Selenium pour les sites n√©cessitant JavaScript\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Scraping {country} avec Selenium...\")\n",
    "    \n",
    "    driver = None\n",
    "    try:\n",
    "        driver = setup_driver(headless=True)\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Attendre le chargement de la page\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Scroll pour charger le contenu dynamique\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Parser le contenu\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        regulations = []\n",
    "        \n",
    "        # Recherche flexible d'articles\n",
    "        items = soup.find_all(['article', 'div', 'li'], \n",
    "                             class_=re.compile('news|item|card|article|regulation', re.I))\n",
    "        \n",
    "        for item in items[:25]:\n",
    "            try:\n",
    "                title_elem = item.find(['h1', 'h2', 'h3', 'h4', 'a'])\n",
    "                if not title_elem:\n",
    "                    continue\n",
    "                    \n",
    "                title = title_elem.get_text(strip=True)\n",
    "                if len(title) < 15:\n",
    "                    continue\n",
    "                \n",
    "                # Date\n",
    "                date_elem = item.find(['time', 'span'], class_=re.compile('date', re.I))\n",
    "                date = date_elem.get_text(strip=True) if date_elem else 'N/A'\n",
    "                \n",
    "                # Lien\n",
    "                link_elem = item.find('a', href=True)\n",
    "                link = link_elem['href'] if link_elem else ''\n",
    "                \n",
    "                regulations.append({\n",
    "                    'Country': country,\n",
    "                    'Authority': authority,\n",
    "                    'Title': title,\n",
    "                    'Date': date,\n",
    "                    'Link': link,\n",
    "                    'Scraped_At': datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        print(f\"‚úÖ {len(regulations)} √©l√©ments trouv√©s\")\n",
    "        return regulations\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "print(\"‚úÖ Fonction Selenium cr√©√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5da68a",
   "metadata": {},
   "source": [
    "## üìä 5. Consolidation et Analyse des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7184da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidation de toutes les donn√©es\n",
    "all_regulations = []\n",
    "\n",
    "if thailand_data:\n",
    "    all_regulations.extend(thailand_data)\n",
    "if singapore_data:\n",
    "    all_regulations.extend(singapore_data)\n",
    "if malaysia_data:\n",
    "    all_regulations.extend(malaysia_data)\n",
    "\n",
    "# Cr√©ation du DataFrame consolid√©\n",
    "df_all = pd.DataFrame(all_regulations)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä R√âSUM√â DES DONN√âES COLLECT√âES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìà Total de r√©gulations: {len(df_all)}\")\n",
    "print(f\"\\nüìç R√©partition par pays:\")\n",
    "print(df_all['Country'].value_counts())\n",
    "print(f\"\\nüèõÔ∏è R√©partition par autorit√©:\")\n",
    "print(df_all['Authority'].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã APER√áU DES DONN√âES\")\n",
    "print(\"=\"*80)\n",
    "display(df_all.head(10))\n",
    "\n",
    "# Statistiques\n",
    "print(\"\\nüìä Informations du dataset:\")\n",
    "print(df_all.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02932c99",
   "metadata": {},
   "source": [
    "## üîç 6. Filtrage et Recherche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc80316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_regulations(df, keyword, country=None):\n",
    "    \"\"\"\n",
    "    Recherche dans les r√©gulations par mot-cl√©\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame des r√©gulations\n",
    "        keyword: Mot-cl√© √† rechercher\n",
    "        country: Pays optionnel pour filtrer\n",
    "    \"\"\"\n",
    "    mask = df['Title'].str.contains(keyword, case=False, na=False)\n",
    "    \n",
    "    if country:\n",
    "        mask &= df['Country'].str.contains(country, case=False, na=False)\n",
    "    \n",
    "    results = df[mask]\n",
    "    \n",
    "    print(f\"üîç R√©sultats pour '{keyword}': {len(results)} trouv√©s\")\n",
    "    return results\n",
    "\n",
    "# Exemples de recherches\n",
    "print(\"üìå Exemple 1: Recherche 'ESG' (Environmental, Social, Governance)\")\n",
    "esg_results = search_regulations(df_all, 'ESG')\n",
    "if len(esg_results) > 0:\n",
    "    display(esg_results[['Country', 'Title', 'Date']].head())\n",
    "\n",
    "print(\"\\nüìå Exemple 2: Recherche 'disclosure'\")\n",
    "disclosure_results = search_regulations(df_all, 'disclosure')\n",
    "if len(disclosure_results) > 0:\n",
    "    display(disclosure_results[['Country', 'Title', 'Date']].head())\n",
    "\n",
    "print(\"\\nüìå Exemple 3: Recherche 'crypto' ou 'digital'\")\n",
    "crypto_mask = df_all['Title'].str.contains('crypto|digital|blockchain', case=False, na=False)\n",
    "crypto_results = df_all[crypto_mask]\n",
    "print(f\"üîç {len(crypto_results)} r√©gulations li√©es aux crypto/digital trouv√©es\")\n",
    "if len(crypto_results) > 0:\n",
    "    display(crypto_results[['Country', 'Title', 'Date']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305e4ecb",
   "metadata": {},
   "source": [
    "## üìä 7. Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d11ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration du style\n",
    "plt.style.use('dark_background')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Cr√©ation des visualisations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('üìä Analyse des Changements R√©glementaires - March√©s Asiatiques', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "# 1. R√©partition par pays\n",
    "country_counts = df_all['Country'].value_counts()\n",
    "axes[0, 0].pie(country_counts.values, labels=country_counts.index, autopct='%1.1f%%',\n",
    "               colors=['#ff6b6b', '#4ecdc4', '#45b7d1'])\n",
    "axes[0, 0].set_title('R√©partition par Pays', fontweight='bold')\n",
    "\n",
    "# 2. Nombre par autorit√©\n",
    "authority_counts = df_all['Authority'].value_counts()\n",
    "axes[0, 1].barh(authority_counts.index, authority_counts.values, \n",
    "                color=['#ff6b6b', '#4ecdc4', '#45b7d1'])\n",
    "axes[0, 1].set_xlabel('Nombre de r√©gulations')\n",
    "axes[0, 1].set_title('R√©gulations par Autorit√©', fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Longueur des titres\n",
    "df_all['Title_Length'] = df_all['Title'].str.len()\n",
    "axes[1, 0].hist(df_all['Title_Length'], bins=20, color='#26de81', alpha=0.7, edgecolor='white')\n",
    "axes[1, 0].set_xlabel('Longueur du titre (caract√®res)')\n",
    "axes[1, 0].set_ylabel('Fr√©quence')\n",
    "axes[1, 0].set_title('Distribution de la Longueur des Titres', fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 4. Top mots-cl√©s\n",
    "import collections\n",
    "all_words = ' '.join(df_all['Title']).lower()\n",
    "words = re.findall(r'\\b[a-z]{4,}\\b', all_words)\n",
    "word_freq = collections.Counter(words)\n",
    "common_words = dict(word_freq.most_common(10))\n",
    "\n",
    "axes[1, 1].barh(list(common_words.keys()), list(common_words.values()),\n",
    "                color='#ffa502')\n",
    "axes[1, 1].set_xlabel('Fr√©quence')\n",
    "axes[1, 1].set_title('Top 10 Mots-Cl√©s', fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualisations cr√©√©es avec succ√®s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b253f364",
   "metadata": {},
   "source": [
    "## üíæ 8. Export des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c0d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export vers CSV\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "csv_filename = f'regulatory_changes_asia_{timestamp}.csv'\n",
    "df_all.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "print(f\"‚úÖ Donn√©es export√©es vers: {csv_filename}\")\n",
    "\n",
    "# Export vers Excel avec formatage\n",
    "excel_filename = f'regulatory_changes_asia_{timestamp}.xlsx'\n",
    "with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
    "    # Sheet 1: Toutes les donn√©es\n",
    "    df_all.to_excel(writer, sheet_name='All Regulations', index=False)\n",
    "    \n",
    "    # Sheet 2: Thailand\n",
    "    df_thailand = df_all[df_all['Country'].str.contains('Thailand')]\n",
    "    if len(df_thailand) > 0:\n",
    "        df_thailand.to_excel(writer, sheet_name='Thailand', index=False)\n",
    "    \n",
    "    # Sheet 3: Singapore\n",
    "    df_singapore = df_all[df_all['Country'].str.contains('Singapore')]\n",
    "    if len(df_singapore) > 0:\n",
    "        df_singapore.to_excel(writer, sheet_name='Singapore', index=False)\n",
    "    \n",
    "    # Sheet 4: Malaysia\n",
    "    df_malaysia = df_all[df_all['Country'].str.contains('Malaysia')]\n",
    "    if len(df_malaysia) > 0:\n",
    "        df_malaysia.to_excel(writer, sheet_name='Malaysia', index=False)\n",
    "    \n",
    "    # Sheet 5: Summary\n",
    "    summary_data = {\n",
    "        'M√©trique': [\n",
    "            'Total R√©gulations',\n",
    "            'Thailand',\n",
    "            'Singapore',\n",
    "            'Malaysia',\n",
    "            'Date de scraping'\n",
    "        ],\n",
    "        'Valeur': [\n",
    "            len(df_all),\n",
    "            len(df_thailand),\n",
    "            len(df_singapore),\n",
    "            len(df_malaysia),\n",
    "            datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        ]\n",
    "    }\n",
    "    pd.DataFrame(summary_data).to_excel(writer, sheet_name='Summary', index=False)\n",
    "\n",
    "print(f\"‚úÖ Donn√©es export√©es vers: {excel_filename}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ EXPORT TERMIN√â\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìÅ Fichiers cr√©√©s:\")\n",
    "print(f\"   - {csv_filename}\")\n",
    "print(f\"   - {excel_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17fa71a",
   "metadata": {},
   "source": [
    "## üîÑ 9. Fonction de Scraping Compl√®te (One-Click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158c6360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_markets(export=True):\n",
    "    \"\"\"\n",
    "    Fonction principale pour scraper tous les march√©s d'un coup\n",
    "    \n",
    "    Args:\n",
    "        export: Si True, exporte automatiquement les r√©sultats\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame consolid√© de toutes les r√©gulations\n",
    "    \"\"\"\n",
    "    print(\"üöÄ D√©marrage du scraping complet...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    # Thailand\n",
    "    try:\n",
    "        th_data = scrape_sec_thailand()\n",
    "        all_data.extend(th_data)\n",
    "        time.sleep(2)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur Thailand: {e}\")\n",
    "    \n",
    "    # Singapore\n",
    "    try:\n",
    "        sg_data = scrape_mas_singapore()\n",
    "        all_data.extend(sg_data)\n",
    "        time.sleep(2)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur Singapore: {e}\")\n",
    "    \n",
    "    # Malaysia\n",
    "    try:\n",
    "        my_data = scrape_sc_malaysia()\n",
    "        all_data.extend(my_data)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur Malaysia: {e}\")\n",
    "    \n",
    "    # Cr√©ation du DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"‚úÖ Scraping termin√©: {len(df)} r√©gulations collect√©es\")\n",
    "    \n",
    "    # Export automatique\n",
    "    if export and len(df) > 0:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f'asia_regulations_{timestamp}.csv'\n",
    "        df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"üíæ Donn√©es export√©es: {filename}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Fonction de scraping compl√®te cr√©√©e\")\n",
    "print(\"üí° Utilisez: df = scrape_all_markets() pour lancer le scraping complet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b0ad6d",
   "metadata": {},
   "source": [
    "## üìù 10. Notes et Am√©liorations Possibles\n",
    "\n",
    "### ‚úÖ Fonctionnalit√©s actuelles:\n",
    "- Scraping de 3 march√©s asiatiques (TH, SG, MY)\n",
    "- Support de Selenium pour sites dynamiques\n",
    "- Export CSV et Excel\n",
    "- Recherche par mots-cl√©s\n",
    "- Visualisations\n",
    "\n",
    "### üöÄ Am√©liorations possibles:\n",
    "1. **Scheduling automatique**: Utiliser `schedule` pour scraper quotidiennement\n",
    "2. **Base de donn√©es**: Stocker dans SQLite/PostgreSQL pour historique\n",
    "3. **Notifications**: Email/Slack quand nouvelles r√©gulations d√©tect√©es\n",
    "4. **NLP**: Analyse de sentiment et extraction d'entit√©s\n",
    "5. **Plus de march√©s**: Hong Kong, Japon, Cor√©e du Sud\n",
    "6. **API REST**: Exposer les donn√©es via Flask/FastAPI\n",
    "7. **Dashboard**: Streamlit ou Dash pour visualisation interactive\n",
    "\n",
    "### ‚ö†Ô∏è Notes importantes:\n",
    "- Respecter les `robots.txt` des sites\n",
    "- Ajouter des delays entre requ√™tes (rate limiting)\n",
    "- V√©rifier r√©guli√®rement si la structure HTML a chang√©\n",
    "- Utiliser des proxies pour √©viter les blocages IP"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
